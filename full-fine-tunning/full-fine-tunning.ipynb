{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b48e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sushobhon/fine-tune-llm/venv_312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07d8c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['The birch canoe slid on the smooth planks.',\n",
       "  'Glue the sheet to the dark blue background.',\n",
       "  \"It's easy to tell the depth of a well.\",\n",
       "  'These days a chicken leg is a rare dish.',\n",
       "  'Rice is often served in round bowls.',\n",
       "  'The juice of lemons makes fine punch.',\n",
       "  'The box was thrown beside the parked truck.',\n",
       "  'The hogs were fed chopped corn and garbage.',\n",
       "  'Four hours of steady work faced us.',\n",
       "  'Large size in stockings is hard to sell.'],\n",
       " 'translation': ['On the smooth planks, the birch canoe slid.',\n",
       "  'Glue the sheet to the dark blue background, you must.',\n",
       "  'Easy it is, to tell the depth of a well.',\n",
       "  'These days, a rare dish, a chicken leg is.',\n",
       "  'In round bowls, rice often served is.',\n",
       "  'Fine punch, the juice of lemons makes.',\n",
       "  'Beside the parked truck, the box was thrown.',\n",
       "  'Chopped corn and garbage, the hogs were fed.',\n",
       "  'Faced us, four hours of steady work did.',\n",
       "  'Hard to sell, large size in stockings is.'],\n",
       " 'translation_extra': ['On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "  'Glue the sheet to the dark blue background, you must.',\n",
       "  'Easy it is, to tell the depth of a well.',\n",
       "  'Hrrmmm. These days, a rare dish, a chicken leg is.',\n",
       "  'In round bowls, rice often served is. Yes, hrrmmm.',\n",
       "  'Fine punch, the juice of lemons makes. Hmm.',\n",
       "  'Beside the parked truck, the box was thrown. Yes, hrrrm.',\n",
       "  'Chopped corn and garbage, the hogs were fed. Hmm.',\n",
       "  'Faced us, four hours of steady work did.',\n",
       "  'Hard to sell, large size in stockings is.']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the dataset.\n",
    "# For this exacise we will be using yoda sentences dataset from Huggingface.\n",
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\")\n",
    "\n",
    "# Viewing the dataset\n",
    "dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a97031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
       " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
       " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       " 'text': 'Sentence: The birch canoe slid on the smooth planks. Translation: On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can not directly use this dataset for training. We need to format the datasets.\n",
    "# we will process the dataset in this form \"Sentence: [English Sentence] Translation: [Yoda Sentence]\"\n",
    "def format_yoda(example):\n",
    "    return {\"text\": f\"Sentence: {example['sentence']} Translation: {example['translation_extra']}\"}\n",
    "\n",
    "dataset = dataset.map(format_yoda)\n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356a5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset to train and evaluation set\n",
    "train_eval_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_eval_split['train'],\n",
    "    'eval': train_eval_split['test']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97491219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load a small pre-trained LLM model and tokenizer\n",
    "# Using 'distilgpt2' as a small model example\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f54fa5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Text ---\n",
      "Sentence: The Sky is clear, it's time to fly.\n",
      "Translation: If you've already been in the game and can't get to the cockpit and get in touch with me, I'm trying to contact you. If\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# # Let's try to translate the sentence using original model.\n",
    "prompt = \"Sentence: The Sky is clear, it's time to fly.\\nTranslation:\" # Include the start of the target sequence\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# Generate text\n",
    "# Adjust generation parameters as needed to control output style\n",
    "output_sequences = model.generate(\n",
    "    input_ids,\n",
    "    max_length=input_ids.shape[1] + 30, # Generate up to 30 new tokens\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,             # To avoid immediate repetition\n",
    "    do_sample=True,                     # Use sampling for more diverse output\n",
    "    top_k=50,                           # Consider top 50 tokens\n",
    "    top_p=0.95,                         # Nucleus sampling\n",
    "    temperature=0.8,                    # Controls randomness (slightly higher for more variation)\n",
    "    pad_token_id=tokenizer.eos_token_id # Ensure generation stops at EOS token\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(\"--- Generated Text ---\")\n",
    "print(generated_text)\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af3ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4dedef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 648/648 [00:00<00:00, 16368.11 examples/s]\n",
      "Map: 100%|██████████| 72/72 [00:00<00:00, 9206.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [31837,\n",
       "  594,\n",
       "  25,\n",
       "  5994,\n",
       "  503,\n",
       "  1115,\n",
       "  8341,\n",
       "  286,\n",
       "  6266,\n",
       "  13,\n",
       "  33322,\n",
       "  25,\n",
       "  5994,\n",
       "  503,\n",
       "  1115,\n",
       "  8341,\n",
       "  286,\n",
       "  6266,\n",
       "  11,\n",
       "  345,\n",
       "  1276,\n",
       "  13],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Preparing the dataset for training (Tokenization)\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text, padding and truncation will be handled by the data collator\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "# Applying tokenize_function and removing other collumns\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"translation\", \"translation_extra\", \"text\"]\n",
    ")\n",
    "\n",
    "# Dataset after tokenization\n",
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc206ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for causal language modeling\n",
    "# This will handle padding and creating the labels (which are the input ids shifted)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d22c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Set up the training arguments\n",
    "output_dir = \"./yoda_finetuned_model\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,              # Location of Output Directory\n",
    "    overwrite_output_dir=True,          # If directory already exists then overwrite\n",
    "    num_train_epochs=5,                 # Number of time the model will be trained on entire data\n",
    "    per_device_train_batch_size=4,      # Number of example will be trained at a time\n",
    "    per_device_eval_batch_size=4,       # Number of example will be evaluated at a time\n",
    "    eval_strategy=\"epoch\",              # Changed from evaluation_strategy\n",
    "    save_strategy=\"epoch\",              # Changed from save_strategy\n",
    "    logging_dir=f\"{output_dir}/logs\",   # Loggs will be saved here\n",
    "    logging_steps=50,                   # Log saving frequently\n",
    "    learning_rate=2e-5,                 \n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,        # Load the best model based on evaluation loss\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "276ccb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5902c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 02:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.136600</td>\n",
       "      <td>1.926351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.866500</td>\n",
       "      <td>1.851621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.698700</td>\n",
       "      <td>1.839700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.616700</td>\n",
       "      <td>1.838783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.600100</td>\n",
       "      <td>1.836944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Start the training process\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61356623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.8369437456130981, 'eval_runtime': 0.6711, 'eval_samples_per_second': 107.291, 'eval_steps_per_second': 26.823, 'epoch': 5.0}\n",
      "Perplexity: 6.277323815417289\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the fine-tuned model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate(eval_dataset=tokenized_datasets[\"eval\"]) # Explicitly pass eval_dataset\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "if 'eval_loss' in eval_results:\n",
    "    perplexity = math.exp(eval_results[\"eval_loss\"])\n",
    "    print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968182d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating example text...\n",
      "--- Generated Text ---\n",
      "Sentence: The Sky is clear, it's time to fly.\n",
      "Translation: To fly, the Sky must. Yes, hrrmmm. Yrsssss. Time to soar, we must, and there is. H\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Example of generating text with the fine-tuned model\n",
    "print(\"Generating example text...\")\n",
    "model.eval() # Set model to evaluation mode. This is a standard practice in pytorch. Here certain layers of the model freezes\n",
    "\n",
    "# Testing for given sentence\n",
    "prompt = \"Sentence: The Sky is clear, it's time to fly.\\nTranslation:\" # Include the start of the target sequence\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# Generate text\n",
    "# Adjust generation parameters as needed to control output style\n",
    "output_sequences = model.generate(\n",
    "    input_ids,\n",
    "    max_length=input_ids.shape[1] + 30, # Generate up to 30 new tokens\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,             # To avoid immediate repetition\n",
    "    do_sample=True,                     # Use sampling for more diverse output\n",
    "    top_k=50,                           # Consider top 50 tokens\n",
    "    top_p=0.95,                         # Nucleus sampling\n",
    "    temperature=0.8,                    # Controls randomness (slightly higher for more variation)\n",
    "    pad_token_id=tokenizer.eos_token_id # Ensure generation stops at EOS token\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(\"--- Generated Text ---\")\n",
    "print(generated_text)\n",
    "print(\"----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to translate the sentence using original model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
